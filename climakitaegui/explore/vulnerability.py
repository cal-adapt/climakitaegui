import numpy as np
import xarray as xr
from climakitaegui.explore import warming_levels
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_load import load
from climakitae.explore.vulnerability import (
    check_params_get_name,
    filter_ba_models,
    get_closest_gridcell_and_separate_files,
    clean_wl_data,
    export_no_e,
    metric_agg,
)
from climakitae.tools.batch import batch_select
from IPython.display import Markdown, display


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    variable,  # must eventually accept temp, precip, or heat index
    approach="time",  # GWL to follow on ## mandatory, Q: Why is anom required for non BC WRF data?
    downscaling_method="Dynamical",  # default for now, statistical to follow ## mandatory
    time_start_year=1981,  # default setting, optional
    time_end_year=2010,  # default setting, optional
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP3-7.0"],
    warming_level="1.5",
    metric_calc="max",
    heat_idx_threshold=None,
    one_in_x=None,
    percentile=None,
    season="all",  ## default to all so no subsetting occurs unless called
    units="degF",  ## default for now
    wrf_bias_adjust=True,
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    batch_mode=False,
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    approach : str
        Approach to follow, default is "time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    units : str, optional
        Units for the retrieved data.
    wrf_bias_adjust : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'off-ramp', 'calculate', 'both', default is 'both'.
    file_format : str, optional
        Export file format options.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    clean_params, names = check_params_get_name(
        **locals()
    )  # This may not be best practice. Also to note, this line MUST be the first line (or at least before other local vars are defined), or else they will also be passed to `check_and_set_params`.

    months_map = {"winter": [12, 1, 2], "summer": [6, 7, 8], "all": np.arange(1, 13)}

    display(Markdown(f"**Calculating {names['var_name']}**"))

    print(f"--- Selecting Data Points --- \n")
    if approach == "time":

        selections = DataParameters()
        selections.data_type = "Gridded"
        selections.downscaling_method = downscaling_method
        selections.scenario_ssp = (
            clean_params["ssp_selected"]
            if historical_data != "Historical Reconstruction"
            else []
        )
        selections.timescale = (
            "hourly" if downscaling_method == "Dynamical" else "daily"
        )  # Toggling time-resolution for WRF and LOCA data respectively
        selections.variable_type = clean_params["variable_type"]
        selections.variable = clean_params["variable"]
        selections.resolution = "3 km"
        selections.units = units
        selections.scenario_historical = [historical_data]
        selections.time_slice = (time_start_year, time_end_year)

        if batch_mode:

            separate_files = False
            locations = input_locations[["lat", "lon"]].values
            print(f"Batch retrieving all {len(locations)} points passed in...\n")
            data_pts = batch_select(selections, locations)

        else:

            data_pts = []
            for index, loc in input_locations.iterrows():

                print(f"Selecting data for {loc['lat'], loc['lon']}")
                lat, lon = loc["lat"], loc["lon"]
                selections.latitude = (
                    lat - 0.02,
                    lat + 0.02,
                )
                selections.longitude = (lon - 0.02, lon + 0.02)

                data = selections.retrieve()

                # Remove leap days, if applicable
                data = data.sel(
                    time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
                )

                # Filter for specified season
                data = data.sel(time=data.time.dt.month.isin(months_map[season]))

                # Filter for BA models
                data = filter_ba_models(
                    data, downscaling_method, wrf_bias_adjust, historical_data
                )

                # Dropping dimensions as approriate
                if historical_data == "Historical Reconstruction":
                    data = data.squeeze(dim="scenario")

                cleaned_data = get_closest_gridcell_and_separate_files(
                    data, lat, lon, downscaling_method, separate_files
                )
                data_pts.append(cleaned_data)

    elif approach == "warming_level":

        data_pts = []
        for index, loc in input_locations.iterrows():

            wl = warming_levels()
            wl.wl_params.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            wl.wl_params.downscaling_method = downscaling_method
            wl.wl_params.variable_type = clean_params["variable_type"]
            wl.wl_params.variable = clean_params["variable"]
            wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
            wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
            wl.wl_params.warming_levels = [
                warming_level
            ]  # Calvin- default, only allow for 1 warming level to be passed in.
            wl.wl_params.units = units
            wl.wl_params.resolution = "3 km"
            wl.wl_params.anom = "No"
            wl.wl_params.months = months_map[season]

            wl.calculate()

            # Clean WL data before using
            data = clean_wl_data(wl.sliced_data[warming_level], downscaling_method)

            # Filter for BA models
            data = filter_ba_models(
                data, downscaling_method, wrf_bias_adjust, historical_data
            )

            # Dropping dimensions as approriate
            if historical_data == "Historical Reconstruction":
                data = data.squeeze(dim="scenario")

            cleaned_data = get_closest_gridcell_and_separate_files(
                data, lat, lon, downscaling_method, separate_files
            )
            data_pts.append(cleaned_data)

    # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
    if approach == "time":
        print("\n--- Loading Data into Memory ---\n")

        if separate_files:
            loaded_data = []
            for point in data_pts:
                print(
                    f"Point ({point.lat.compute().item()}, {point.lon.compute().item()})"
                )
                data = load(point, progress_bar=True)
                loaded_data.append(data)

        else:
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

    elif approach == "warming_level":

        # Don't load or print anything since warming levels are already loaded
        loaded_data = data_pts

    # Export raw data if desired
    if export_method == "off-ramp" or export_method == "both":

        print("\n--- Offramp Exporting Raw Data ---")

        if downscaling_method == "Statistical":
            print(
                "\nExporting Statistical data in most granular time availability (daily)\n"
            )

        if separate_files:
            for pt_idx in range(len(loaded_data)):
                export_no_e(
                    loaded_data[pt_idx],
                    filename=names["offramp_name"],
                    format=file_format,
                )
        else:
            export_no_e(loaded_data, filename="combined_raw_data", format=file_format)

        if export_method == "off-ramp":
            return loaded_data

    print("\n--- Calculating Metrics ---")
    print("Calculating... ", end="", flush=True)

    if separate_files:
        metric_data = []
        # Calculate and export into separate files
        for point in loaded_data:
            calc_val = metric_agg(
                point,
                approach,
                metric_calc,
                heat_idx_threshold,
                one_in_x,
                percentile,
                names["var_name"],
            )
            metric_data.append(calc_val)

    # Export a combined file of all metrics
    else:
        metric_data = metric_agg(
            loaded_data,
            approach,
            metric_calc,
            heat_idx_threshold,
            one_in_x,
            percentile,
            names["var_name"],
        )
    print("Complete!")

    # Export the data
    print("\n--- Exporting Metric Data ---")

    if export_method == "calculate" or export_method == "both":

        if separate_files:
            for pt_idx in range(len(metric_data)):
                export_no_e(
                    metric_data[pt_idx],
                    filename=f"{names['calc_name']}_{str(lat).replace('.', '')}N_{str(lon).replace('.', '')}W",
                    format=file_format,
                )  # Will need to include naming convention for calculated file too.

        else:
            export_no_e(metric_data, filename="metric_data", format=file_format)

        # Returning values to user
        if len(metric_data) == 1:
            metric_data = metric_data[0]
        if len(loaded_data) == 1:
            loaded_data = loaded_data[0]

        if export_method == "calculate":
            return {"calc_data": metric_data}
        elif export_method == "both":
            return {"calc_data": metric_data, "offramp_data": loaded_data}

    elif export_method == "None":  # Specific for table generation
        return {"calc_data": metric_data}
