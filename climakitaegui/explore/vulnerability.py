import numpy as np
import xarray as xr
import param
from climakitaegui.explore import warming_levels
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_load import load
from climakitae.explore.vulnerability import (
    CavaParams,
    filter_ba_models,
    clean_wl_data,
    export_no_e,
    metric_agg,
)
from climakitae.util.utils import (
    get_closest_gridcell,
    stack_sims_across_locs
)
from climakitae.tools.batch import batch_select
from IPython.display import Markdown, display


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    variable,  # must eventually accept temp, precip, or heat index
    units=None,  ## default for now
    approach="time",  # GWL to follow on ## mandatory, Q: Why is anom required for non BC WRF data?
    downscaling_method="Dynamical",  # default for now, statistical to follow ## mandatory
    time_start_year=1981,  # default setting, optional
    time_end_year=2010,  # default setting, optional
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP3-7.0"],
    warming_level="1.5",
    metric_calc="max",
    heat_idx_threshold=None,
    one_in_x=None,
    percentile=None,
    season="all",  ## default to all so no subsetting occurs unless called
    wrf_bias_adjust=True,
    export_method="both",  # raw, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    batch_mode=False,
    distr="gev",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    approach : str
        Approach to follow, default is "time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    units : str, optional
        Units for the retrieved data.
    wrf_bias_adjust : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'raw', 'calculate', 'both', default is 'both'.
    file_format : str, optional
        Export file format options.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    with param.exceptions_summarized():
        params = CavaParams(**locals())
        clean_params = params.get_names()

        months_map = {
            "winter": [12, 1, 2],
            "summer": [6, 7, 8],
            "all": np.arange(1, 13),
        }

        locations = input_locations[["lat", "lon"]].values

        display(Markdown(f"**Calculating {clean_params['var_name']}**"))

        print(f"--- Selecting Data Points --- \n")
        if approach == "time":

            selections = DataParameters()
            selections.data_type = "Gridded"
            selections.downscaling_method = downscaling_method
            selections.scenario_ssp = (
                clean_params["ssp_selected"]
                if historical_data != "Historical Reconstruction"
                else []
            )
            selections.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            selections.variable_type = clean_params["variable_type"]
            selections.variable = clean_params["variable"]
            selections.resolution = "3 km"
            selections.units = units
            selections.scenario_historical = [historical_data]
            selections.time_slice = (time_start_year, time_end_year)

            if batch_mode:
                separate_files = False
                data_pts = batch_select(selections, locations, "time")

            else:

                data_pts = []
                for index, loc in input_locations.iterrows():

                    print(f"Selecting data for {loc['lat'], loc['lon']}")
                    lat, lon = loc["lat"], loc["lon"]
                    selections.latitude = (
                        lat - 0.02,
                        lat + 0.02,
                    )
                    selections.longitude = (lon - 0.02, lon + 0.02)

                    data = selections.retrieve()

                    # Remove leap days, if applicable
                    data = data.sel(
                        time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
                    )

                    # Filter for specified season
                    data = data.sel(time=data.time.dt.month.isin(months_map[season]))

                    # Filter for BA models
                    data = filter_ba_models(
                        data, downscaling_method, wrf_bias_adjust, historical_data
                    )

                    # Dropping dimensions as approriate
                    if historical_data == "Historical Reconstruction":
                        data = data.squeeze(dim="scenario")

                    data = get_closest_gridcell(data, lat, lon, print_coords=False)
                    data = stack_sims_across_locs(data, "simulation")

                    data_pts.append(data)

        elif approach == "warming_level":
            wl = warming_levels()
            wl.wl_params.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            wl.wl_params.downscaling_method = downscaling_method
            wl.wl_params.variable_type = clean_params["variable_type"]
            wl.wl_params.variable = clean_params["variable"]
            wl.wl_params.warming_levels = [
                warming_level
            ]  # Calvin- default, only allow for 1 warming level to be passed in.
            wl.wl_params.units = units
            wl.wl_params.resolution = "3 km"
            wl.wl_params.anom = "No"
            wl.wl_params.months = months_map[season]

            # Band-aid
            if batch_mode and downscaling_method == "Statistical":
                print(
                    "Batch mode for Statistical data in Warming Level approach is not optimized for multiple locations (in development). Resetting `batch_mode` to False. This may take some time because each location is retrieved and returned separately.\n"
                )
                batch_mode = False

            if batch_mode:

                wl.wl_params.load_data = False
                separate_files = False
                data_pts = batch_select(wl, locations, "warming_level")

                # Clean WL data before using
                data_pts = clean_wl_data(data_pts, downscaling_method, separate_files)

                # Filter for BA models
                data_pts = filter_ba_models(
                    data_pts, downscaling_method, wrf_bias_adjust, historical_data
                )

            else:

                data_pts = []
                for index, loc in input_locations.iterrows():
                    print(f"Selecting data for {loc['lat'], loc['lon']}")
                    lat, lon = loc["lat"], loc["lon"]

                    wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
                    wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
                    wl.calculate()

                    # Clean WL data before using
                    data = clean_wl_data(
                        wl.sliced_data[warming_level],
                        downscaling_method,
                        separate_files,
                    )

                    # Filter for BA models
                    data = filter_ba_models(
                        data, downscaling_method, wrf_bias_adjust, historical_data
                    )

                    # # Dropping dimensions as approriate
                    # if historical_data == "Historical Reconstruction":
                    #     data = data.squeeze(dim="scenario")

                    data = get_closest_gridcell(data, lat, lon, print_coords=False)
                    data = stack_sims_across_locs(data, "simulation")

                    data_pts.append(data)

        # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
        if approach == "time":
            print("\n--- Loading Data into Memory ---\n")

            if separate_files:
                loaded_data = []
                for point in data_pts:
                    print(
                        f"Point ({point.lat.compute().item()}, {point.lon.compute().item()})"
                    )
                    data = load(point, progress_bar=True)
                    loaded_data.append(data)

            else:
                data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
                loaded_data = load(data_pts, progress_bar=True)

        elif approach == "warming_level":

            # Don't load or print anything since warming levels are already loaded
            loaded_data = data_pts

        # Export raw data if desired
        if export_method == "raw" or export_method == "both":

            print("\n--- Exporting Raw Data ---")

            if downscaling_method == "Statistical":
                print(
                    "\nExporting Statistical data in most granular time availability (daily)\n"
                )

            if separate_files:
                for pt_idx in range(len(loaded_data)):
                    export_no_e(
                        loaded_data[pt_idx],
                        filename=clean_params["raw_name"],
                        format=file_format,
                    )
            else:
                export_no_e(
                    loaded_data, filename="combined_raw_data", format=file_format
                )

            if export_method == "raw":
                return loaded_data

        print("\n--- Calculating Metrics ---")
        print("Calculating... ", end="", flush=True)

        if separate_files:
            metric_data = []
            # Calculate and export into separate files
            for point in loaded_data:
                calc_val = metric_agg(
                    point,
                    approach,
                    metric_calc,
                    heat_idx_threshold,
                    one_in_x,
                    percentile,
                    clean_params["var_name"],
                    distr,
                )
                metric_data.append(calc_val)

        # Export a combined file of all metrics
        else:
            metric_data = metric_agg(
                loaded_data,
                approach,
                metric_calc,
                heat_idx_threshold,
                one_in_x,
                percentile,
                clean_params["var_name"],
                distr,
            )
        print("Complete!")

        # Export the data
        print("\n--- Exporting Metric Data ---")

        if export_method == "calculate" or export_method == "both":

            if separate_files:
                for pt_idx in range(len(metric_data)):
                    export_no_e(
                        metric_data[pt_idx],
                        filename=f"{clean_params['calc_name']}_{str(metric_data[pt_idx].lat.item()).replace('.', '')}N_{str(metric_data[pt_idx].lon.item()).replace('.', '')}W",
                        format=file_format,
                    )  # Will need to include naming convention for calculated file too.

            else:
                export_no_e(metric_data, filename="metric_data", format=file_format)

            # Returning values to user
            if len(metric_data) == 1:
                metric_data = metric_data[0]
            if len(loaded_data) == 1:
                loaded_data = loaded_data[0]

            if export_method == "calculate":
                return {"calc_data": metric_data}
            elif export_method == "both":
                return {"calc_data": metric_data, "raw_data": loaded_data}

        elif export_method == "None":  # Specific for table generation
            print("Data export selection set to 'None'; no data is exported!")
            return {"calc_data": metric_data}
